import google.generativeai as genai
import os
from typing import List, Dict
from .data_processor import DataProcessor

class ChatbotHandler:
    """
    Maneja las interacciones con Google Gemini API
    """
    
    def __init__(self):
        self.data_processor = DataProcessor()
        self.data_loaded = self.data_processor.load_data()
        
        # Configurar Google Gemini
        api_key = os.getenv("GOOGLE_API_KEY")
        
        if api_key:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-pro')
            self.api_available = True
        else:
            self.api_available = False
            print("‚ö†Ô∏è GOOGLE_API_KEY no configurada")
        
        # Sistema de prompts
        self.system_prompt = self._build_system_prompt()
        
    def _build_system_prompt(self) -> str:
        """Construye el prompt del sistema con contexto de datos"""
        
        base_prompt = """Eres un asistente virtual especializado en an√°lisis de seguridad ciudadana para el Observatorio de Seguridad de Santander, Colombia.

Tu funci√≥n es ayudar a funcionarios p√∫blicos, investigadores y ciudadanos a comprender los datos de criminalidad y las predicciones generadas por modelos de machine learning.

CARACTER√çSTICAS:
- Respondes en espa√±ol de manera clara y profesional
- Usas datos espec√≠ficos cuando est√°n disponibles
- Explicas conceptos t√©cnicos de forma accesible
- Eres preciso y evitas especulaciones
- Cuando no tienes informaci√≥n espec√≠fica, lo indicas claramente

"""
        
        if self.data_loaded:
            data_context = self.data_processor.get_context_string()
            base_prompt += f"\n{data_context}\n"
        else:
            base_prompt += "\nNOTA: Los datos a√∫n no est√°n cargados. Informa al usuario que debe cargar los archivos CSV.\n"
        
        base_prompt += """
INSTRUCCIONES:
1. Si te preguntan sobre estad√≠sticas espec√≠ficas, usa los datos del contexto
2. Si te preguntan sobre predicciones, explica que fueron generadas con modelos ML
3. Si te preguntan sobre metodolog√≠a, explica Random Forest y an√°lisis temporal
4. Mant√©n respuestas concisas (m√°ximo 4-5 p√°rrafos)
5. Usa emojis ocasionalmente para hacer la conversaci√≥n m√°s amigable

Responde siempre de manera √∫til y basada en datos.
"""
        
        return base_prompt
    
    def get_response(self, user_message: str, max_tokens: int = 500) -> str:
        """
        Genera una respuesta usando Google Gemini
        """
        
        if not self.api_available:
            return self._fallback_response(user_message)
        
        try:
            # Construir el prompt completo
            full_prompt = f"{self.system_prompt}\n\nUsuario: {user_message}\n\nAsistente:"
            
            # Configurar generaci√≥n
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=0.7,
                top_p=0.95,
            )
            
            # Generar respuesta usando Gemini
            response = self.model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            
            # Obtener texto de respuesta
            if response.text:
                cleaned_response = self._clean_response(response.text)
                return cleaned_response
            else:
                return self._fallback_response(user_message)
            
        except Exception as e:
            print(f"Error generando respuesta con Gemini: {e}")
            
            # Respuesta de fallback sin LLM
            return self._fallback_response(user_message)
    
    def _clean_response(self, response: str) -> str:
        """Limpia y formatea la respuesta del LLM"""
        # Remover texto repetido o cortado
        response = response.strip()
        
        # Asegurar que termina en punto
        if response and not response[-1] in ['.', '!', '?']:
            # Buscar el √∫ltimo punto
            last_period = response.rfind('.')
            if last_period > len(response) * 0.7:  # Si est√° en el √∫ltimo 30%
                response = response[:last_period + 1]
        
        return response
    
    def _fallback_response(self, user_message: str) -> str:
        """
        Respuesta de emergencia cuando el LLM falla
        Usa l√≥gica simple basada en palabras clave
        """
        
        user_message_lower = user_message.lower()
        
        # Respuestas por palabras clave
        if any(word in user_message_lower for word in ['hola', 'buenos d√≠as', 'buenas tardes', 'hey']):
            return """¬°Hola! üëã Soy el asistente virtual del Observatorio de Seguridad de Santander. 

Puedo ayudarte con:
üìä Estad√≠sticas de criminalidad
üéØ Predicciones de seguridad
üó∫Ô∏è Informaci√≥n por municipio
üìà An√°lisis de tendencias

¬øQu√© te gustar√≠a saber?"""
        
        elif any(word in user_message_lower for word in ['municipio', 'municipios', 'ciudad', 'pueblos']):
            return f"""El Observatorio cubre los **87 municipios** de Santander, incluyendo:

üèõÔ∏è **√Årea Metropolitana:** Bucaramanga, Floridablanca, Gir√≥n, Piedecuesta
üåÑ **Provincias:** Comunera, Garc√≠a Rovira, Guanent√°, Mares, Soto, V√©lez

Los datos incluyen an√°lisis hist√≥ricos y predicciones basadas en machine learning para cada municipio.

¬øSobre qu√© municipio espec√≠fico te gustar√≠a informaci√≥n?"""
        
        elif any(word in user_message_lower for word in ['predicci√≥n', 'predicciones', 'futuro', 'proyecci√≥n']):
            return """üéØ **Predicciones de Seguridad**

Utilizamos modelos de Machine Learning (Random Forest) para predecir:
- Nivel de riesgo por municipio
- Tendencias de criminalidad
- Zonas de mayor incidencia
- Patrones temporales

Las predicciones se basan en:
‚úÖ M√°s de 1 mill√≥n de registros hist√≥ricos
‚úÖ Variables temporales y geoespaciales
‚úÖ Patrones de criminalidad identificados

¬øQuieres saber sobre alg√∫n municipio espec√≠fico?"""
        
        elif any(word in user_message_lower for word in ['dato', 'datos', 'estad√≠stica', 'estad√≠sticas']):
            if self.data_loaded:
                return self.data_processor.get_summary()
            else:
                return """üìä **Informaci√≥n de Datos**

El sistema est√° preparado para analizar:
- Datos hist√≥ricos de criminalidad
- Predicciones generadas por ML
- Informaci√≥n de 87 municipios

‚ö†Ô∏è Los archivos CSV a√∫n no est√°n cargados. Por favor, aseg√∫rate de que los archivos est√©n en la carpeta `data/`.

¬øNecesitas ayuda con la carga de datos?"""
        
        elif any(word in user_message_lower for word in ['funciona', 'c√≥mo', 'qu√© es', 'explicar']):
            return """ü§ñ **Sobre este Observatorio**

Este sistema combina:
- **An√°lisis de Big Data:** Procesamiento de millones de registros
- **Machine Learning:** Modelos predictivos de seguridad
- **Visualizaci√≥n:** Dashboards interactivos en Power BI
- **IA Conversacional:** Este chatbot para consultas

**Tecnolog√≠as:**
üêç Python (Pandas, Scikit-learn)
üìä Power BI
ü§ñ Hugging Face (Mistral AI)
‚òÅÔ∏è Streamlit Cloud

Desarrollado para la Gobernaci√≥n de Santander üèõÔ∏è

¬øQu√© m√°s te gustar√≠a saber?"""
        
        elif any(word in user_message_lower for word in ['ayuda', 'help', 'qu√© puedes hacer']):
            return """üí¨ **¬øC√≥mo puedo ayudarte?**

Puedes preguntarme sobre:

1Ô∏è‚É£ **Estad√≠sticas generales**
   - "¬øCu√°ntos delitos se registraron?"
   - "Dame estad√≠sticas de criminalidad"

2Ô∏è‚É£ **Informaci√≥n por municipio**
   - "¬øQu√© tal la seguridad en Bucaramanga?"
   - "Municipios m√°s seguros"

3Ô∏è‚É£ **Predicciones**
   - "¬øQu√© predicen los modelos?"
   - "Tendencias de seguridad"

4Ô∏è‚É£ **Metodolog√≠a**
   - "¬øC√≥mo funcionan las predicciones?"
   - "¬øQu√© datos usan?"

¬°Escribe tu pregunta! üòä"""
        
        else:
            return """Gracias por tu pregunta. En este momento estoy procesando informaci√≥n limitada sin conexi√≥n al modelo principal.

üìä **Puedo ayudarte con:**
- Informaci√≥n general del observatorio
- Explicaci√≥n de la metodolog√≠a
- Estad√≠sticas b√°sicas (si los datos est√°n cargados)

Para consultas m√°s espec√≠ficas, intenta:
- Cargar los archivos CSV en la carpeta `data/`
- Configurar GOOGLE_API_KEY en los Secrets de Streamlit

¬øHay algo espec√≠fico sobre el observatorio que quieras saber?"""
        
        return response
    
    def get_data_summary(self) -> str:
        """Retorna resumen de datos disponibles"""
        if self.data_loaded:
            return self.data_processor.get_summary()
        else:
            return "‚ö†Ô∏è No hay datos cargados. Agrega archivos CSV en la carpeta 'data/'."
